
# 深度残差学习为图像识别

*Kaiming He, Xianyu Zhang, Shaoqing Ren, Jian Sun*

*Microsoft Research*

## 摘要

越深的神经网络越难训练。我们提出一个残差学习框架，使训练比那些先前提出的网络还要深的多的网络变得容易。我们利用------我们提供具有竞争力的经验证据显示这些残差网络是易于优化的，且可以从大大增加的深度中获得精度。在ImageNet数据集中，我们评估残差网络以一个深度达到152层——8倍于VGG网络深度，但是仍旧有更低的复杂度。一套这些残差网络获得了3.57%误差在ImageNet测试集中。这个结果赢得了第一的位置在ILSVRC 2015分类任务中。我们也提供了在CIFAR-10中100-1000层的分析。

## 1. 导言

深度卷积神经网络[22, 21]已经导致一系列的突破在图片分类中。深度网络自然地整合低/中/高级别特征和分类器，以一个端到端的多层方式，且特征的“等级”能够被丰富，通过一定数量的被堆叠的层（深度）。最近的证据[41，44]揭示了网络深度是至关重要的，且导致的结果在挑战ImageNet数据集中都利用“非常深”的模型，深度从16[41]到30[16]。很多其他的非凡的视觉识别任务[8,12,7,32,27]也从非常深的模型中获益。

受至关重要的深度驱动，一个问题冒了出来：学习更好的网络像叠更多层一样简单吗？一个障碍来回答这个问题就是臭名昭著的梯度消失/爆炸问题[1, 9]，这阻碍了从开始的收敛。这个问题，然而，被极大地缓解通过初始归一化[23, 9, 37, 13]和中间的归一化层[16]，这使得几十层的网络靠随机梯度下降和反向传播开始收敛。

当更深的网络能够开始收敛，一个退化问题被暴露出来：随着网络深度的增长，精度开始饱和（或许也不奇怪），然后快速降低。未预料到的是，这个退化不是由过拟合引起的，且增加更多的层到一个适当深度的模型会导致更高的训练误差，如[11, 42]中的报告，且通过我们的实验彻底验证了。图1，展示了一个典型的例子。

![](img\resnet_fig1_train_error.png "")

图1. 训练误差（左）和测试误差（右边）在CIFAR-10中，用20层和56层“白板”网络。更深的网络有更高的训练误差，和这样的测试误差。相似的现象在ImageNet中被呈现在图4中。

（训练精度的）退化表明不是所有的系统都是同样易于优化的。让我们考虑一个浅的架构，和它增加更多层的更深版本。存在一个解决通过构造更深的模型：增加的层是恒等映射，其它层是从被学习的浅模型中拷贝而来。这个构造的方案的存在证明了一个更深的模型不应该产生比相应浅模型更高的训练误差。但是经验展示我们当前手上的解决者无法找到比被构造的解决一样好或更好的解决（或者不能在可行的时间中办到）。

在这个论文中，我们通过引入一个深度残差学习框架来缓解问题。抛弃寄希望于每个少量堆叠的层直接拟合一个残余映射。正式地，记想要的底层映射为$\mathcal H(x)$，我们让堆叠的非线性层拟合另一个映射$\mathcal F(x):= \mathcal H(x) - x$。原始的映射被重写为$\mathcal F(x)+x$。我们假设优化残差映射相较于优化原始的无引用映射更容易。走向极端，如果一个恒等映射优化好了，相较于通过一个非线性层的堆叠拟合一个恒等映射，将残差逼成0更容易。

*理解：$\mathcal H(x)$是理想的“只会更好”的网络，$\mathcal F(x)$是实际可学习的层。*

![](img\resnet_fig2_block.png "")

公式$F(x)+x$能够被实现通过前向神经网络和“短接”（图2）。短接[2, 34, 49]指的是那些“跳过”层或更多其它层。在本例中，短接简单地执行恒等映射，且它们的输出被加到堆叠的层的输出中（图2）。恒等短接不增加额外的参数及计算复杂度。整个网络依然能够以端到端、SGD和反向传播来训练，且能够依靠常规库（如Caffe）轻易地被实现，不需要修改解决者。

我们做了全面的实验在ImageNet上来展示退化问题，并评估我们的方法。我们展示：1）我们的极深残差网络是易于优化的，但是对应的“白板”网络（简单叠层）展现出更高的训练误差当深度增加时；2）我们的深度残差网络能够轻易地提高精度通过大幅增加深度，产出的结果大幅好于之前的网络。

相同的现象也出现在CIFAR-10数据集中，这表明优化困难和我们的方法的有效性不局限于特定数据集。我们呈现成功地训练100层以上的模型在这个数据集中，并探索了超过1000层的模型。

在ImageNet分类数据集中，我们获得了极好的结果，靠极深的残差网络。我们的152层残差网络是用在ImageNet中最深的网络，但是仍旧比VGG网络的复杂度低。我们的套装有3.57%的top-5误差在ImageNet测试集中，并且赢得了第一的位置在ILSVRC 2015分类竞赛中。极深的表征也拥有出色的泛化性能在其他识别任务中，并且让我们进一步赢得了第一的位置在：ImageNet检测，ImageNet定位，COCO检测和COCO分割在ILSVRC和COCO 2015竞赛中。这强有力证据展现了残差学习原则是通用的，且我们预计它适用在其它视觉和非视觉问题中。

## 2. 相关工作

**残差表征** 在图像识别中，VLAD[18]是一个表征，它参照字典通过残差向量编码，Fisher Vector[30]能够用公式表示为一个概率版本[18]的VLAD。它们都是强大的浅层表征为图像检索和分类[4, 48]。对于向量化，编码残差向量展现出更有效，相较于编码原始向量。

在低级别视觉和计算图中，为解决偏微分方程（PDEs），广泛使用的多栅（Multigrid）方法[3]重新表示系统作为子问题在多尺度，每个子问题负责残差解决在粗糙和精细尺度。多栅的一个替代品是分层偏置预条件[45, 46]，它依赖于表征两尺度之间残差向量的变量。已经证明[3, 45, 46]这些解决者相较于标准解决者（没有意识到方案的残差性质）收敛更快。这些方法表明一个好的重形式或预条件能够简化优化。

*解读：这里主要讲了以前的残差是怎么做的，并表明残差可以使得训练变得容易。*


**短接** 实践和理论导致短接[2, 34, 49]已经被研究了很长时间。一个早期的训练多层感知机的实践是增加一个线性层连接网络的输入和输出[34, 49]。在[44, 24]中，一部分中间层被直接连接到辅助分类器为了解决梯度消失/爆炸。论文[39, 38, 31, 47]提出方法为中心层回复、梯度和传播误差，通过短接实现。在[44]中，一个“inception”层由短接分支和一些更深的分支组成。

与我们的工作并行的，“高速网络(highway network)”[42, 43]提出带控制功能的短接[15]。这些门（gates）是依靠数据的且有参数，相较于我们的恒等短接，我们的不需要参数。当一个门短接（gated shortcut）被“关闭”（接近0），高速网络中的层代表非残差函数。与之相反的是，我们的形式一直学习残差函数；我们的恒等短接从不关闭，且所有信息一直被输送过去，带有额外的被学习的残差函数。另外，高速网络没有证明在急剧增加深度（如超过100层）时也能提高精度。

*解读：短接以前也有，但是我们的方法无额外参数、一直学，且很深的网络中也有效。*

## 3. 深度残差学习

### 3.1 残差学习
让我们认为$\mathcal H(x)$是一个底层映射（underlying mapping）通过一些堆叠的层拟合（不需要整个网络），x记为第一个层的输入。如果一个假设，多非线性层能够逐渐近似复杂函数，那么它相当于假设它们能逐渐近似残差功能，如 $\mathcal H(x)-x$（假设输入和输出维度相同）。所以与其期望堆叠层去近似 $\mathcal H(x)$，不如我们利用这些层近似一个残差函数 $\mathcal F(x):= \mathcal H(x) - x$。原始函数因此成为 $\mathcal F(x)+x$。即使两个形式都能够逐渐近似为想要的函数（像之前假设的），但是学习的难度或许不同。

*解读：直接“白板学习”残差功能$\mathcal H(x)$理论上也可行，但是训练起来比“手动残差”$\mathcal F(x)+x$难度要大。*

在实际案例中，不太可能恒等映射是最优的，但是我们的形式或许帮助预处理（precondition）问题。如果最优函数更接近恒等映射而不是零映射（zero mapping），它应该更容易对于解决者去寻找扰动在恒等映射的参考下，然后学习函数作为一个新的。我们展示通过实验（图7），经过学习的残差函数在通常情况下有小的反馈，表明恒等映射提供了合理的预处理。

![](img\resnet_fig7_std.png "")
Fig.7 层反馈的标准差在CIFAR-10中。反馈是每个3×3层的输出，在BN后非线性层之前。上图：原始顺序的层反馈标准差。下图：根据降序排列后的层反馈标准差（省得忽高忽低，比较直观）。

*解读：图7表明用了ResNet后网络参数更稳定，标准差小。*

*解读：实际情况中，恒等映射（直接+x）或许不是最优的方案，但是如果最优的方案更接近恒等映射而不是零映射（啥也不加），那么直接+x就应该比啥也不加要好。*

### 3.2 依靠短接的恒等映射
我们对每个堆叠层采用残差学习。一个构建块被展示在图2中。形式上，在本文中我们将一个构建块定义为：

$$y = \mathcal F(x, \{w_i\}) + x \tag 1$$

这里 $x$ 和 $y$ 是输入和输出向量。函数 $\mathcal F(x, \{w_i\})$ 表示待学习的残差映射。如图2案例所示，有两个层，

## 4. 实验


